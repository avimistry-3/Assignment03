{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment03.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMy1lhCJTIDWbQ37kuA5mYX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avimistry-3/Assignment03/blob/main/Assignment03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwdBl5yrfGJo",
        "outputId": "b40424bd-e2ee-4abd-9319-4a1d72144dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import brown\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from nltk.corpus import stopwords \n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "from scipy import spatial\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ],
      "metadata": {
        "id": "dxnMapzJoyiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "data1=(brown.sents())\n",
        "print(data1)\n",
        "data2=(movie_reviews.sents())\n",
        "print(data2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUUkw-aHfbJt",
        "outputId": "62c7ac7a-06a8-46fb-8c1b-cd6a13154779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n",
            "[['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.'], ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie=[]\n",
        "stop_words = set(stopwords.words('english')) \n",
        "for sent in movie_reviews.sents():\n",
        "    sent= [w for w in sent if not w in stop_words]\n",
        "    sent = ' '.join(sent)\n",
        "    sent=sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    movie.append(sent)\n",
        "for i in range(20):\n",
        "  print(movie[i])\n",
        "    \n",
        "for i, sent in enumerate(movie):\n",
        "    movie[i] = sent.split()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEqhCZv3pU6I",
        "outputId": "5ae73e92-923e-427b-d527-873a8710e19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "plot  two teen couples go church party  drink drive \n",
            "get accident \n",
            "one guys dies  girlfriend continues see life  nightmares \n",
            " deal \n",
            "watch movie  sorta  find \n",
            "\n",
            "\n",
            "critique  mind  fuck movie teen generation touches cool idea  presents bad package \n",
            "makes review even harder one write  since generally applaud films attempt break mold  mess head  lost highway  memento   good bad ways making types films  folks  snag one correctly \n",
            "seem taken pretty neat concept  executed terribly \n",
            "problems movie \n",
            "well  main problem  simply jumbled \n",
            "starts  normal  downshifts  fantasy  world  audience member  idea  going \n",
            "dreams  characters coming back dead  others look like dead  strange apparitions  disappearances  looooot chase scenes  tons weird things happen  simply explained \n",
            "personally  mind trying unravel film every  give clue  get kind fed  film  biggest problem \n",
            " obviously got big secret hide  seems want hide completely final five minutes \n",
            "make things entertaining  thrilling even engaging  meantime \n",
            "really \n",
            "sad part arrow dig flicks like  actually figured half  way point  strangeness start make little bit sense  still  make film entertaining \n",
            "guess bottom line movies like always make sure audience   even given secret password enter world understanding \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown=[]\n",
        "stop_words = set(stopwords.words('english')) \n",
        "for sent in data1:\n",
        "    sent= [w for w in sent if not w in stop_words]\n",
        "    sent = ' '.join(sent)\n",
        "    sent=sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    brown.append(sent) \n",
        "for i in range(10):\n",
        "  print(brown[i])\n",
        "for i, sent in enumerate(brown):\n",
        "    brown[i] = sent.split()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l32OlX8prcao",
        "outputId": "b4dc689d-2649-48db-d3a6-1ad666f15d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fulton County Grand Jury said Friday investigation Atlantas recent primary election produced  evidence  irregularities took place \n",
            "The jury said termend presentments City Executive Committee  overall charge election   deserves praise thanks City Atlanta  manner election conducted \n",
            "The SeptemberOctober term jury charged Fulton Superior Court Judge Durwood Pye investigate reports possible  irregularities  hardfought primary Mayornominate Ivan Allen Jr \n",
            " Only relative handful reports received   jury said   considering widespread interest election  number voters size city  \n",
            "The jury said find many Georgias registration election laws  outmoded inadequate often ambiguous  \n",
            "It recommended Fulton legislators act  laws studied revised end modernizing improving  \n",
            "The grand jury commented number topics  among Atlanta Fulton County purchasing departments said  well operated follow generally accepted practices inure best interest governments  \n",
            "Merger proposed\n",
            "However  jury said believes  two offices combined achieve greater efficiency reduce cost administration  \n",
            "The City Purchasing Department  jury said   lacking experienced clerical personnel result city personnel policies  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processCorpus(train):\n",
        "    traincorp = []\n",
        "    for i in train:\n",
        "        traincorp.append(word_tokenize(i))\n",
        "    return traincorp"
      ],
      "metadata": {
        "id": "vYnViMrltDVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=str(brown)\n",
        "traincorp = processCorpus(train)\n",
        "modelW2V = Word2Vec(traincorp, min_count=1)\n",
        "modelW2V.save('model1.bin')\n"
      ],
      "metadata": {
        "id": "IeLC3o6xEZHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "foxcbEYCL-1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "n794Oo-JzsLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown=[]\n",
        "for sent in data1:\n",
        "    sent = ' '.join(sent)\n",
        "    sent=sent.translate(str.maketrans('', '', string.punctuation))\n",
        "    brown.append(sent)   \n",
        "for i, sent in enumerate(brown):\n",
        "    brown[i] = sent.split()"
      ],
      "metadata": {
        "id": "kzYKWXCQ7DQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_doc_contain_word = Counter()\n",
        "for doc in brown:\n",
        "    count_words = Counter(doc)\n",
        "    total_doc_contain_word.update(count_words)"
      ],
      "metadata": {
        "id": "x79aIhYVzzi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate TF for each word and doc\n",
        "all_tf = []\n",
        "for doc in brown:\n",
        "    tf=[]\n",
        "    total_words = len(doc)\n",
        "    for word in total_doc_contain_word:\n",
        "        if word not in doc:\n",
        "            tf.append(0)\n",
        "        else:\n",
        "            tf.append(count_words[word] / total_words)\n",
        "    all_tf.append(tf)"
      ],
      "metadata": {
        "id": "BlknrX097NaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate IDF for each word\n",
        "idf = []\n",
        "all_doc = len(brown)\n",
        "for word in total_doc_contain_word:\n",
        "    thisidf = np.log(all_doc / (total_doc_contain_word[word]+1))\n",
        "    idf.append(thisidf)"
      ],
      "metadata": {
        "id": "LgO2z7jJ7cAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate TF-IDF \n",
        "doc_tfidf = []\n",
        "for doc in all_tf:\n",
        "    tf_idf = [a*b for a,b in zip(doc,idf)]\n",
        "    doc_tfidf.append(tf_idf)"
      ],
      "metadata": {
        "id": "U3EnlGRG8-Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape tfidf matrix to make it word * doc\n",
        "tfidf = np.array(doc_tfidf)\n",
        "tfidf = [*zip(*tfidf)]\n",
        "tfidf = np.array(tfidf)"
      ],
      "metadata": {
        "id": "O4Ayqqkh9DnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nDCG(gold_set, model):\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "        gold_set, {'ndcg'})\n",
        "\n",
        "    x = evaluator.evaluate(temp)\n",
        "    item= []\n",
        "    for i in x.items():\n",
        "        item.append((i[1]['ndcg']))"
      ],
      "metadata": {
        "id": "8uh7yG7ZMbyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath= '/content/drive/MyDrive/SimLex-999.txt'"
      ],
      "metadata": {
        "id": "Euj66_WNpcyW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}